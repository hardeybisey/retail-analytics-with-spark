######################################
## Minio Job
######################################
apiVersion: batch/v1
kind: Job
metadata:
  name: minio-setup-job
  namespace: airflow
  labels:
    component: minio-setup-job
spec:
  ttlSecondsAfterFinished: 300
  template:
    metadata:
      labels:
        component: minio-setup-job
    spec:
      serviceAccountName: airflow-sa
      securityContext:
        runAsUser: 0
        runAsGroup: 0
        fsGroup: 0
      restartPolicy: OnFailure
      volumes:
        - name: init-data
          emptyDir: {}
      initContainers:
        - name: data-generator
          image: iadebisi/ecommerce-data-generator
          imagePullPolicy: IfNotPresent
          restartPolicy: OnFailure
          args: ["--customers", "2000", "--sellers", "200", "--orders", "200000", "--format", "parquet", "--output_dir", "/tmp/data"]
          volumeMounts:
            - name: init-data
              mountPath: /tmp/data
      containers:
        - name: minio-client
          image: minio/mc
          imagePullPolicy: IfNotPresent
          restartPolicy: OnFailure
          command: ["/bin/sh", "-c"]
          volumeMounts:
            - name: init-data
              mountPath: /tmp/data
              readOnly: true
          args:
            - |
              sleep 5 &&
              mc alias set local http://minio-server-svc:9000 ${AWS_ACCESS_KEY_ID} ${AWS_SECRET_ACCESS_KEY};
              mc mb --ignore-existing local/${S3_LOGS_BUCKET};
              mc mb --ignore-existing local/${S3_INPUTS_BUCKET};
              mc mb --ignore-existing local/${S3_WH_BUCKET};
              mc mb --ignore-existing local/${S3_STG_BUCKET};
              mc cp --recursive /tmp/data/*.parquet local/${S3_INPUTS_BUCKET}/;
              echo "keep" | mc pipe local/${S3_LOGS_BUCKET}/spark-events/.keep;
          envFrom:
            - configMapRef:
                name: airflow-config
            - secretRef:
                name: airflow-secrets
