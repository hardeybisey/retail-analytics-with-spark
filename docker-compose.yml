services:
  spark-master:
    container_name: spark-master
    build: spark/
    command: master
    environment:
      - SPARK_MODE=master
      - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}
      - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY}
      - AWS_REGION=${AWS_REGION}
      - SPARK_PUBLIC_DNS=${SPARK_PUBLIC_DNS}
      - SPARK_SSL_ENABLED=no
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_MASTER_HOST=spark-master
    ports:
      - "7077:7077"       # Spark master port
      - "8080:8080"       # Spark Master UI
      - "18080:18080"   # History server UI
    networks:
      spark-net:
    depends_on:
      - minio
    volumes:
      - spark-events:/home/iceberg/spark-events
    deploy:
      resources:
        limits:
          cpus: 1
          memory: 512m


  spark-worker-one:
    build: spark/
    command: worker
    container_name: spark-worker-one
    environment:
      - SPARK_MODE=worker
      - SPARK_WORKER_MEMORY=${SPARK_WORKER_MEMORY}
      - SPARK_WORKER_CORES=${SPARK_WORKER_CORES}
      - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}
      - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY}
      - AWS_REGION=${AWS_REGION}
      - SPARK_PUBLIC_DNS=${SPARK_PUBLIC_DNS}
    depends_on:
      - spark-master
    ports:
      - "8081:8081" # Spark Worker UI
    networks:
      spark-net:
    volumes:
      - spark-events:/home/iceberg/spark-events

  spark-worker-two:
    build: spark/
    command: worker
    container_name: spark-worker-two
    environment:
      - SPARK_MODE=worker
      - SPARK_WORKER_MEMORY=${SPARK_WORKER_MEMORY}
      - SPARK_WORKER_CORES=${SPARK_WORKER_CORES}
      - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}
      - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY}
      - AWS_REGION=${AWS_REGION}
      - SPARK_PUBLIC_DNS=${SPARK_PUBLIC_DNS}
    depends_on:
      - spark-master
    ports:
      - "8082:8081" # Spark Worker UI
    networks:
      spark-net:
    volumes:
      - spark-events:/home/iceberg/spark-events

  spark-driver:
    build: spark/
    command: driver
    container_name: spark-driver
    environment:
      - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}
      - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY}
      - AWS_REGION=${AWS_REGION}
      - SPARK_MODE=driver
      - SPARK_DRIVER_HOST=spark-driver
      - SPARK_DRIVER_MEMORY=${SPARK_DRIVER_MEMORY}
      - SPARK_DRIVER_CORES=${SPARK_DRIVER_CORES}
      - SPARK_EXECUTOR_MEMORY=${SPARK_EXECUTOR_MEMORY}
      - SPARK_EXECUTOR_CORES=${SPARK_EXECUTOR_CORES}
      - SPARK_EXECUTOR_INSTANCES=${SPARK_EXECUTOR_INSTANCES}
      - SPARK_PUBLIC_DNS=${SPARK_PUBLIC_DNS}
      - PYSPARK_DRIVER_PYTHON=jupyter-notebook
      - PYSPARK_DRIVER_PYTHON_OPTS=--ip=0.0.0.0 --NotebookApp.token='' --NotebookApp.${AWS_SECRET_ACCESS_KEY}='' --port=8888 --no-browser --allow-root
    depends_on:
      - spark-master
      - spark-worker-one
      - spark-worker-two
    ports:
      - "8888:8888"
      - "4040:4040"
    networks:
      spark-net:
    volumes:
      - spark-events:/home/iceberg/spark-events
    deploy:
      resources:
        limits:
          cpus: 1
          memory: 1G

  minio:
    image: minio/minio
    container_name: minio
    environment:
      - MINIO_ROOT_USER=${MINIO_ROOT_USER}
      - MINIO_ROOT_PASSWORD=${MINIO_ROOT_PASSWORD}
      - MINIO_DOMAIN=${MINIO_DOMAIN}
    ports:
      - "9000:9000"
      - "9001:9001"
    networks:
      spark-net:
        aliases:
          - warehouse.minio
    volumes:
      - minio-data:/data
    command: server /data --console-address ":9001"
    deploy:
      resources:
        limits:
          cpus: 1
          memory: 2G

  mc:
    image: minio/mc
    depends_on:
      - minio
    environment:
      - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}
      - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY}
    networks:
      spark-net:
    volumes:
      - ./csv_input:/tmp/data
    entrypoint: >
      /bin/sh -c "
      sleep 5 &&
      mc alias set local http://minio:9000 ${AWS_ACCESS_KEY_ID} ${AWS_SECRET_ACCESS_KEY};
      mc mb --ignore-existing local/${S3_LOGS_BUCKET};
      mc mb --ignore-existing local/${S3_INPUTS_BUCKET};
      mc mb --ignore-existing local/${S3_WH_BUCKET};
      mc cp --recursive /tmp/data/*.csv local/${S3_INPUTS_BUCKET}/;
      exit 0;
      "
    restart: "no"

  iceberg-rest:
    image: tabulario/iceberg-rest
    container_name: iceberg-rest
    environment:
      - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}
      - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY}
      - AWS_REGION=${AWS_REGION}
      - CATALOG_WAREHOUSE=s3://${S3_WH_BUCKET}/
      - CATALOG_IO__IMPL=org.apache.iceberg.aws.s3.S3FileIO
      - CATALOG_S3_ENDPOINT=http://minio:9000
    ports:
      - "8181:8181"
    networks:
      spark-net:
    depends_on:
      - minio
    deploy:
      resources:
        limits:
          cpus: 1
          memory: 1G

volumes:
  minio-data:
  spark-events:

networks:
  spark-net:
    driver: bridge
