---
x-spark-common:
  &spark-common
  # build: docker/spark/
  image: iadebisi/iceberg-spark
  env_file:
    - .env
  volumes:
    - ./docker/spark/conf:/opt/spark/conf

name: spark
services:
  # data-gen:
  #   image: iadebisi/ecommerce-data-generator
  #   command: >
  #     --customers 20000
  #     --sellers 2000
  #     --orders 2000000
  #     --format parquet
  #   volumes:
  #       - ./raw_input:/data

  spark-master:
    <<: *spark-common
    container_name: spark-master
    command: master
    environment:
      # <<: *spark-common-env
      SPARK_MODE: master
      SPARK_SSL_ENABLED: no
      SPARK_RPC_AUTHENTICATION_ENABLED: no
      SPARK_RPC_ENCRYPTION_ENABLED: no
      SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED: no
      SPARK_MASTER_HOST: 0.0.0.0
    ports:
      - "7077:7077"       # Spark master port
      - "8081:8080"       # Spark Master UI
      - "18080:18080"   # History server UI
    networks:
      spark-net:
      shared-network:
        # ipv4_address: 172.80.0.10
    depends_on:
      - minio
    # deploy:
    #   resources:
    #     limits:
    #       cpus: 1
    #       memory: 512m


  spark-worker-one:
    <<: *spark-common
    container_name: spark-worker-one
    command: worker
    environment:
      # <<: *spark-common-env
      SPARK_MODE: worker
      SPARK_WORKER_CORES: 2
      SPARK_WORKER_MEMORY: 2G
    depends_on:
      - spark-master
    ports:
      - "8082:8081" # Spark Worker UI
    networks:
      spark-net:
      shared-network:
        # ipv4_address: 172.80.0.11
    # deploy:
    #   resources:
    #     limits:
    #       cpus: 2
    #       memory: 2G

  spark-worker-two:
    <<: *spark-common
    container_name: spark-worker-two
    command: worker
    environment:
      # <<: *spark-common-env
      SPARK_MODE: worker
      SPARK_WORKER_CORES: 6
      SPARK_WORKER_MEMORY: 6G
    depends_on:
      - spark-master
    ports:
      - "8083:8081" # Spark Worker UI
    networks:
      spark-net:
      shared-network:
        # ipv4_address: 172.80.0.12
    # deploy:
    #   resources:
    #     limits:
    #       cpus: 6
    #       memory: 6G


  # spark-notebook:
  #   <<: *spark-common
  #   container_name: spark-notebook
  #   command: >
  #     jupyter notebook
  #     --ip=0.0.0.0
  #     --NotebookApp.token=''
  #     --NotebookApp.password=''
  #     --port=8888
  #     --no-browser
  #     --allow-root
  #   environment:
  #     # <<: *spark-common-env
  #     SPARK_MODE: driver
  #     SPARK_DRIVER_HOST: spark-notebook
  #   ports:
  #     - "8888:8888"
  #     - "4041:4040"
  #   networks:
  #     spark-net:
  #   volumes:
  #     - ./notebooks:/home/iceberg/notebooks
    # deploy:
    #   resources:
    #     limits:
    #       cpus: 1
    #       memory: 1G

  minio:
    image: minio/minio
    container_name: minio
    healthcheck:
      test: ["CMD", "curl", "-f", "http://minio:9000/minio/health/ready"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    environment:
      MINIO_ROOT_USER: ${AWS_ACCESS_KEY_ID}
      MINIO_ROOT_PASSWORD: ${AWS_SECRET_ACCESS_KEY}
      MINIO_DOMAIN: minio
    ports:
      - "9000:9000"
      - "9001:9001"
    networks:
      shared-network:
        # ipv4_address: 172.80.0.13
        aliases:
          - ${S3_WH_BUCKET}.minio
      spark-net:
        aliases:
          - ${S3_WH_BUCKET}.minio
    volumes:
      - minio-data:/data
    command: server /data --console-address ":9001"
    # deploy:
    #   resources:
    #     limits:
    #       cpus: 1
    #       memory: 2G

  minio-console:
    image: opens3/console:latest
    container_name: opens3-console
    ports:
      - "9090:9090"
    networks:
      spark-net:
    environment:
      CONSOLE_MINIO_SERVER: http://minio:9000
      CONSOLE_PORT: 9090
    depends_on:
      - minio

  minio-client:
    image: minio/mc
    depends_on:
      minio:
        condition: service_healthy
      # data-gen:
      #   condition: service_completed_successfully
    environment:
      AWS_ACCESS_KEY_ID: ${AWS_ACCESS_KEY_ID}
      AWS_SECRET_ACCESS_KEY: ${AWS_SECRET_ACCESS_KEY}
    networks:
      spark-net:
    volumes:
      - ./raw_input:/tmp/data
    entrypoint: >
      /bin/sh -c "
      sleep 5 &&
      mc alias set local http://minio:9000 ${AWS_ACCESS_KEY_ID} ${AWS_SECRET_ACCESS_KEY};
      mc mb --ignore-existing local/${S3_LOGS_BUCKET};
      mc mb --ignore-existing local/${S3_INPUTS_BUCKET};
      mc mb --ignore-existing local/${S3_WH_BUCKET};
      mc mb --ignore-existing local/${S3_STG_BUCKET};
      mc cp --recursive /tmp/data/*.parquet local/${S3_INPUTS_BUCKET}/;
      echo "keep" | mc pipe local/${S3_LOGS_BUCKET}/spark-events/.keep;
      exit 0;
      "
    restart: "no"

  # hive-metastore:
  #   image: apache/hive:standalone-metastore-4.1.0
  #   container_name: hive-metastore
  #   environment:
  #     SERVICE_NAME: metastore
  #     DB_DRIVER: postgres
  #     SERVICE_OPTS: >-
  #       -Djavax.jdo.option.ConnectionDriverName=org.postgresql.Driver
  #       -Djavax.jdo.option.ConnectionURL=jdbc:postgresql://postgres:5432/${HIVE_DB}
  #       -Djavax.jdo.option.ConnectionUserName=${PG_USER}
  #       -Djavax.jdo.option.ConnectionPassword=${PG_PASS}
  #   ports:
  #     - "9083:9083"
  #   networks:
  #     - spark-net
  #     - shared-network
  #   # depends_on:
  #   #   - postgres
  #   volumes:
  #     - type: bind
  #       source: ./postgresql-42.5.1.jar
  #       target: /opt/hive/lib/postgres.jar

  # postgres:
  #   image: postgres:16
  #   container_name: hive-postgres
  #   environment:
  #     POSTGRES_USER: ${PG_USER}
  #     POSTGRES_PASSWORD: ${PG_PASS}
  #     POSTGRES_DB: ${HIVE_DB}
  #   ports:
  #     - "5432:5432"
  #   volumes:
  #     - hive-db:/var/lib/postgresql/data
  #   networks:
  #     - spark-net
  #     - shared-network


  iceberg-rest:
    image: tabulario/iceberg-rest
    container_name: iceberg-rest
    environment:
      AWS_ACCESS_KEY_ID: ${AWS_ACCESS_KEY_ID}
      AWS_SECRET_ACCESS_KEY: ${AWS_SECRET_ACCESS_KEY}
      AWS_REGION: ${AWS_REGION}
      CATALOG_WAREHOUSE: s3://${S3_WH_BUCKET}/
      CATALOG_S3_ENDPOINT: http://minio:9000
      CATALOG_IO__IMPL: org.apache.iceberg.aws.s3.S3FileIO
    ports:
      - "8181:8181"
    networks:
      spark-net:
      shared-network:
        # ipv4_address: 172.80.0.14
    depends_on:
      - minio
    # deploy:
    #   resources:
    #     limits:
    #       cpus: 1
    #       memory: 512m

volumes:
  minio-data:
  # hive-db:

networks:
  spark-net:
    driver: bridge
  shared-network:
    driver: bridge
    external: true
